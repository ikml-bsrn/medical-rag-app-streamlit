def initialise_rag_chain(hf_embedding_model_name="NeuML/pubmedbert-base-embeddings", k=3):
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_chroma import Chroma
    from langchain_core.runnables import RunnablePassthrough
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser
    from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace

    # initialise HuggingFace embedding model for embeddings
    embeddings = HuggingFaceEmbeddings(model_name=hf_embedding_model_name)

    # load the existing Chroma vector store
    vector_store = Chroma(
        collection_name="medquad_collection",
        persist_directory="medquad_chroma_db",
        embedding_function=embeddings
    )

    # initialise retriever from the vector store
    retriever = vector_store.as_retriever(
        search_type="similarity",
        search_kwargs={"k": k})
    
    # create the prompt template
    template = """
                You are a helpful medical assistant. Your job is to use the provided context to answer the question as accurately as possible.
                If the context does not contain the answer, respond politely with "I'm sorry, I do not currently have that information."

                Question: {question}

                Context: {context}

                Answer:
                """

    # initiate the prompt using ChatPromptTemple
    prompt = ChatPromptTemplate.from_template(template)
    
    # initialise the base LLM model
    hf_llm_base = HuggingFaceEndpoint(
        repo_id="openai/gpt-oss-20b",
        provider="together"
    )
    # wrap the model with Chat model class
    hf_chat_model = ChatHuggingFace(llm=hf_llm_base)

    # create the RAG chain with LCEL
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()} # retrieves the context
        | prompt                                                  # creates the prompt with the context
        | hf_chat_model                                           # calls the LLM
        | StrOutputParser()
    )

    return rag_chain

async def rag_retrieve_by_query(rag_chain, query, hf_embedding_model_name="NeuML/pubmedbert-base-embeddings", k=3):
    """
    This function retrieves the top-k relevant documents from a vector store based on the input query 
    and streams the response generated by the RAG chain.

    Args:
        query (str): The input query for which relevant documents are to be retrieved.
        hf_embedding_model_name (str): The name of the HuggingFace embedding model to use.
        k (int): The number of top relevant documents to retrieve.
    
    Returns:
        str: The response generated by the RAG chain based on the retrieved documents.
    """
    # stream the RAG chain with the input query
    async for chunk in rag_chain.astream({"question": query}):
        yield chunk